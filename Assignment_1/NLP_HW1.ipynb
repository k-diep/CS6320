{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg4Gq6CoacaB",
        "outputId": "ee4838df-fbd1-49a6-aac8-e3f63b2510f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of the file on Google Drive\n",
        "file_path = '/content/drive/MyDrive/NLP_Data/anat19.txt'"
      ],
      "metadata": {
        "id": "PQH4C_6HbEL2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TBD - SYSTEM ARGUMENT\n",
        "\n",
        "# Open the file and read its contents\n",
        "with open(file_path, 'r') as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "# Print the first 1000 characters of raw_text\n",
        "# print(raw_text[:1000])"
      ],
      "metadata": {
        "id": "NlWF55Q5gQ1J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing NLTK\n",
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEmBXFSdi-4m",
        "outputId": "a995d602-cd5b-42c7-f90e-b0df5008f084"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Tokenize the raw text\n",
        "tokenized_raw_text = word_tokenize(raw_text)\n",
        "\n",
        "# Display the first 10 tokens\n",
        "print(tokenized_raw_text[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eJ2Geohjw2F",
        "outputId": "b9557a5b-3c1b-4006-dd36-7111868230e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Heart', 'Anatomy', 'The', 'vital', 'importance', 'of', 'the', 'heart', 'is', 'obvious']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical diversity code adapted from https://github.com/kjmazidi/NLP/blob/master/Part_2-Words/Chapter_05_words/5.1_Words1.ipynb\n",
        "\n",
        "# Finding the total number of tokens\n",
        "print(\"\\nThe number of tokens in anat19.txt: \", len(tokenized_raw_text))\n",
        "\n",
        "# Finding the total number of unique tokens\n",
        "unique_tokens = set(tokenized_raw_text)\n",
        "print(\"\\nThe number of unique tokens in anat19.txt:\", len(unique_tokens))\n",
        "\n",
        "# Printing the first 5 unique tokens\n",
        "print(\"\\nThe first 5 unique tokens in anat19.txt:\", list(unique_tokens)[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSw75BNEkBdZ",
        "outputId": "386bb853-c3fb-4f90-e782-b94629ae184f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The number of tokens in anat19.txt:  20218\n",
            "\n",
            "The number of unique tokens in anat19.txt: 3096\n",
            "\n",
            "The first 5 unique tokens in anat19.txt: ['primitive', 'leaflets', 'inexpensive', 'prognosis', 'makes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating lexical diversity\n",
        "print(\"\\nLexical diversity of anat19.txt: %.2f\" % (len(unique_tokens) / len(tokenized_raw_text)))"
      ],
      "metadata": {
        "id": "csPqigWskVPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c71422-ba58-4144-ca83-f4b2fb3ac830"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lexical diversity of anat19.txt: 0.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(raw_text):\n",
        "\n",
        "  # Tokenize the raw text and lowercase\n",
        "  tokenized_raw_text = word_tokenize(raw_text.lower())\n",
        "  # print(tokenized_raw_text)\n",
        "\n",
        "  # Filter out tokens to tokens that are alpha\n",
        "  alpha_tokens = [token for token in tokenized_raw_text if token.isalpha()]\n",
        "  # print(alpha_tokens)\n",
        "\n",
        "  # Filter out tokens to tokens that not in the in the stopword list\n",
        "  stopwords_tokens = [token for token in alpha_tokens if token not in stopwords.words('english')]\n",
        "  # print(stopwords_tokens)\n",
        "\n",
        "  # Filter out tokens to tokens with a length > 5\n",
        "  length_greater_than_5_tokens = [token for token in stopwords_tokens if len(token) > 5]\n",
        "  # print(length_greater_than_5_tokens)\n",
        "\n",
        "  # Lemmatize the tokens\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in length_greater_than_5_tokens]\n",
        "  # print(lemmatized_tokens)\n",
        "\n",
        "  # Create a set to get unique lemmas\n",
        "  unique_lemmatized_tokens = set(lemmatized_tokens)\n",
        "  # print(unique_lemmatized_tokens)\n",
        "  # print(len(unique_lemmatized_tokens))\n",
        "\n",
        "  # Do POS tagging on the unique lemmas\n",
        "  tags = pos_tag(unique_lemmatized_tokens)\n",
        "  print(\"The first 20 words and their tag:\", tags[:20])\n",
        "\n",
        "  # Filter the unique lemmas to only nouns\n",
        "  nouns = [word for word, tag in tags if tag == 'NN']\n",
        "  #print(nouns)\n",
        "\n",
        "  # Printing the number of non unique tokens from step a.\n",
        "  print(\"Number of tokens that are alpha, not in the NLTK stopword list, and have length > 5: \", len(length_greater_than_5_tokens), \"\\n\")\n",
        "\n",
        "  # Printing the number of nouns.\n",
        "  print(\"Number of nouns: \", len(nouns), \"\\n\")\n",
        "\n",
        "  # Misunderstanding of the instructions - Scrapped this code\n",
        "  # --------------------------------------------------------------\n",
        "  # Get a random non unique token\n",
        "  # random_non_unique_token = random.choice(length_greater_than_5_tokens)\n",
        "  # print(random_non_unique_token)\n",
        "\n",
        "  # Get the first random noun\n",
        "  # random_noun_1 = random.choice(nouns)\n",
        "  # print(random_noun_1)\n",
        "\n",
        "  # Get the second random noun\n",
        "  # random_noun_2 = random.choice(nouns)\n",
        "  # print(random_noun_2)\n",
        "  # --------------------------------------------------------------\n",
        "\n",
        "  # Return list of non unique tokens, and list of nouns\n",
        "  return length_greater_than_5_tokens, nouns\n",
        "\n",
        "tokens, nouns = preprocess_text(raw_text)\n",
        "\n",
        "# Creating a dictionary of {noun: count of noun}\n",
        "noun_count_dictionary = {noun: tokens.count(noun) for noun in nouns}\n",
        "# print(noun_count_dictionary)\n",
        "\n",
        "# Adapted this code to sort https://www.geeksforgeeks.org/different-ways-of-sorting-dictionary-by-values-and-reverse-sorting-by-values/#\n",
        "# Sorting the noun\n",
        "noun_count_sorted = sorted(noun_count_dictionary.items(), key = lambda kv: kv[1], reverse=True)\n",
        "# print(noun_count_sorted)\n",
        "\n",
        "# Initialized the list of nouns\n",
        "nouns_list = []\n",
        "\n",
        "# Printing the 50 most common words and their counts\n",
        "for key_value in noun_count_sorted[:50]:\n",
        "\n",
        "  # Extract the noun, count from the tuple (key_value)\n",
        "  common_noun, common_noun_count = key_value\n",
        "\n",
        "  # Print the sorted nouns and their counts\n",
        "  print(common_noun, \":\", common_noun_count)\n",
        "  nouns_list.append(common_noun)\n",
        "\n",
        "# print(nouns_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtMw3atqocO0",
        "outputId": "3e0954d9-a1e5-4b99-b47c-d429a2e5b2bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 20 words and their tag: [('primitive', 'JJ'), ('prognosis', 'NN'), ('inexpensive', 'JJ'), ('training', 'NN'), ('tensing', 'VBG'), ('report', 'NN'), ('administering', 'VBG'), ('stimulus', 'JJ'), ('forceful', 'JJ'), ('technique', 'NN'), ('serf', 'NN'), ('elapsed', 'VBD'), ('drastically', 'RB'), ('mitosis', 'JJ'), ('formation', 'NN'), ('bypass', 'NN'), ('leading', 'VBG'), ('revealed', 'JJ'), ('delicate', 'JJ'), ('thyroid', 'NN')]\n",
            "Number of tokens that are alpha, not in the NLTK stopword list, and have length > 5:  7020 \n",
            "\n",
            "Number of nouns:  665 \n",
            "\n",
            "muscle : 75\n",
            "contraction : 70\n",
            "pressure : 43\n",
            "increase : 33\n",
            "stimulation : 33\n",
            "septum : 32\n",
            "include : 27\n",
            "calcium : 26\n",
            "depolarization : 24\n",
            "oxygen : 24\n",
            "percent : 22\n",
            "system : 22\n",
            "conduction : 20\n",
            "semilunar : 20\n",
            "diastole : 19\n",
            "tissue : 18\n",
            "opening : 17\n",
            "activity : 17\n",
            "contract : 16\n",
            "chordae : 16\n",
            "function : 16\n",
            "pericardium : 16\n",
            "myocardium : 16\n",
            "minute : 16\n",
            "period : 15\n",
            "patient : 14\n",
            "disease : 13\n",
            "exercise : 13\n",
            "supply : 13\n",
            "treatment : 13\n",
            "pattern : 13\n",
            "potential : 13\n",
            "circulation : 13\n",
            "contractility : 13\n",
            "sulcus : 12\n",
            "trigger : 11\n",
            "portion : 11\n",
            "surgery : 11\n",
            "volume : 11\n",
            "repolarization : 11\n",
            "decrease : 10\n",
            "mechanism : 10\n",
            "cavity : 10\n",
            "stretch : 10\n",
            "rhythm : 10\n",
            "vessel : 10\n",
            "sodium : 10\n",
            "region : 10\n",
            "relaxation : 10\n",
            "strength : 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition: Print word with spaces in between\n",
        "def print_word_with_spaces(word):\n",
        "  for letter in word:\n",
        "    print(letter, \"\", end='')"
      ],
      "metadata": {
        "id": "p-WpEJ8HJ07K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_game(nouns_list):\n",
        "  # Choose a random noun in the list\n",
        "  random_noun = random.choice(nouns_list)\n",
        "\n",
        "  # Uncomment to see the random noun\n",
        "  print(\"Random Noun: \", random_noun)\n",
        "\n",
        "  # Initial Points\n",
        "  points = 5\n",
        "\n",
        "  # Create the current guessed words (which is _ * length of the word)\n",
        "  current_guessed_word = []\n",
        "  for i in range(len(random_noun)):\n",
        "    current_guessed_word.append(\"_\")\n",
        "\n",
        "  print_word_with_spaces(current_guessed_word)\n",
        "\n",
        "  while(1):\n",
        "    if points < 0:\n",
        "      print(\"\\nGAME OVER, you ran out of points.\")\n",
        "      break\n",
        "\n",
        "    # Check to see if the user has won\n",
        "    elif '_' not in current_guessed_word:\n",
        "      print(\"\\nCongrats. You've won with\", points, \"points left.\")\n",
        "      break\n",
        "    else:\n",
        "      # Ask the user for a letter\n",
        "      user_letter = input(\"\\nGuess a letter (enter \\'!\\' to exit): \")\n",
        "\n",
        "      # Checking if the input is \"!\" and exiting if it is\n",
        "      if user_letter == \"!\":\n",
        "        print(\"Exiting now.\")\n",
        "        break\n",
        "\n",
        "      # Checking if the input is an alpha char with size 1 (one letter)\n",
        "      elif not user_letter.isalpha() or len(user_letter)!=1:\n",
        "        print(\"\\nPlease input one letter character.\\nScore is\", points)\n",
        "\n",
        "      # Checking if user has already guessed that letter\n",
        "      elif user_letter in current_guessed_word:\n",
        "        print(\"\\nYou already guessed that letter. Try again.\\nScore is\", points)\n",
        "\n",
        "      # Checking if letter is in the random noun\n",
        "      elif user_letter in random_noun:\n",
        "        points += 1\n",
        "        print(\"\\nRight!\\nScore is\", points)\n",
        "\n",
        "        # Replacing current_guessed_word with the current guess letter\n",
        "        for letter_index in range(len(random_noun)):\n",
        "          if random_noun[letter_index] == user_letter:\n",
        "            current_guessed_word[letter_index] = random_noun[letter_index]\n",
        "\n",
        "        # print(current_guessed_word)\n",
        "\n",
        "      else:\n",
        "        points -= 1\n",
        "        print(\"Sorry, guess again. Score is\", points)\n",
        "\n",
        "      # Print the current word\n",
        "      print_word_with_spaces(current_guessed_word)"
      ],
      "metadata": {
        "id": "swkPooHjf4i1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C8OrYAtZ29f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Driver of game\n",
        "# One instance of the game\n",
        "print(\"Let's play a word guessing game!\")\n",
        "word_game(nouns_list)\n",
        "\n",
        "\n",
        "# Prompt\n",
        "while(1):\n",
        "  user_prompt = input(\"\\nDo you want to play again? Enter \\'@\\' if No. Enter anything else if Yes: \")\n",
        "  if user_prompt == \"@\":\n",
        "    print(\"Exiting now.\")\n",
        "    break\n",
        "  else:\n",
        "    print(\"Guess another noun.\")\n",
        "    word_game(nouns_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvPfqbmUUJHV",
        "outputId": "aa9acd90-cf24-4807-9e55-2061e81ff80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's play a word guessing game!\n",
            "Random Noun:  semilunar\n",
            "_ _ _ _ _ _ _ _ _ \n",
            "Guess a letter (enter '!' to exit): s\n",
            "\n",
            "Right!\n",
            "Score is 6\n",
            "s _ _ _ _ _ _ _ _ \n",
            "Guess a letter (enter '!' to exit): e\n",
            "\n",
            "Right!\n",
            "Score is 7\n",
            "s e _ _ _ _ _ _ _ \n",
            "Guess a letter (enter '!' to exit): m\n",
            "\n",
            "Right!\n",
            "Score is 8\n",
            "s e m _ _ _ _ _ _ \n",
            "Guess a letter (enter '!' to exit): i\n",
            "\n",
            "Right!\n",
            "Score is 9\n",
            "s e m i _ _ _ _ _ \n",
            "Guess a letter (enter '!' to exit): l\n",
            "\n",
            "Right!\n",
            "Score is 10\n",
            "s e m i l _ _ _ _ \n",
            "Guess a letter (enter '!' to exit): u\n",
            "\n",
            "Right!\n",
            "Score is 11\n",
            "s e m i l u _ _ _ "
          ]
        }
      ]
    }
  ]
}